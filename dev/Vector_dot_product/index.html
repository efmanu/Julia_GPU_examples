<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Health Warning: Types · Julia_GPU_examples.</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Julia_GPU_examples.</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><span class="toctext">Getting Started</span><ul><li><a class="toctext" href="../GPU_background/">Some Background on GPUs</a></li><li><a class="toctext" href="../Accessing_GPUs/">Accessing GPUs</a></li><li><a class="toctext" href="../Setup_Julia/">Set-up Julia</a></li></ul></li><li><span class="toctext">Developing With GPUs</span><ul><li><a class="toctext" href="../Vector_addition/">An Introduction to Parallelism</a></li><li class="current"><a class="toctext" href>Health Warning: Types</a><ul class="internal"><li class="toplevel"><a class="toctext" href="#Shared-Memory-and-Synchronisation-1">Shared Memory and Synchronisation</a></li><li class="toplevel"><a class="toctext" href="#Vector-Dot-Product-1">Vector Dot Product</a></li><li class="toplevel"><a class="toctext" href="#synchronise-threads-1">synchronise threads</a></li><li class="toplevel"><a class="toctext" href="#In-the-step-below,-we-add-up-all-of-the-values-stored-in-the-cache-1">In the step below, we add up all of the values stored in the cache</a></li><li class="toplevel"><a class="toctext" href="#cache[1]-now-contains-the-sum-of-vector-dot-product-calculations-done-in-1">cache[1] now contains the sum of vector dot product calculations done in</a></li><li class="toplevel"><a class="toctext" href="#this-block,-so-we-write-it-to-c-1">this block, so we write it to c</a></li><li class="toplevel"><a class="toctext" href="#A-Note-on-Static-and-Dynamic-Allocation-1">A Note on Static and Dynamic Allocation</a></li></ul></li><li><a class="toctext" href="../Streaming/">Streaming</a></li></ul></li><li><span class="toctext">Performance</span><ul><li><a class="toctext" href="../Profiling/">Profiling</a></li><li><a class="toctext" href="../Performance_thoughts/">Thoughts on Performance</a></li></ul></li><li><a class="toctext" href="../Summary/">Summary</a></li><li><a class="toctext" href="../Further_Reading/">Further Reading</a></li><li><a class="toctext" href="../About_the_author/">About the Author</a></li></ul></nav><article id="docs"><header><nav><ul><li>Developing With GPUs</li><li><a href>Health Warning: Types</a></li></ul><a class="edit-page" href="github.com/jenni-westoby/Julia_GPU_examples.git"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Health Warning: Types</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Health-Warning:-Types-1" href="#Health-Warning:-Types-1">Health Warning: Types</a></h1><p>The vector addition example in the previous section was written in fairly high level code. We didn&#39;t have to think about memory allocation or even types. As we move on to more involved examples in this section and beyond, this is increasingly going to change. Julia has an optional types system, which in normal Julia usage often means that in practice you can ignore that types exist. It is often not possible to ignore types in Julia GPU programming. If you are a bit hazy about what types are, I recommend doing a bit of background reading from the Julia manual before proceeding. I will assume you know what types are here.</p><h1><a class="nav-anchor" id="Shared-Memory-and-Synchronisation-1" href="#Shared-Memory-and-Synchronisation-1">Shared Memory and Synchronisation</a></h1><p>Following our example of vector addition in the previous section, you may be left wondering what the point of making a distinction between blocks and threads is. This section should make this clear.</p><p>You may recall from &quot;Some Background on GPUs&quot; that GPUs are composed of grids of blocks, where each block contains threads.</p><p>Put a pretty picture here.</p><p>In addition to threads, each block contains &#39;shared memory&#39;. Shared memory is memory which can be read and written to by all the threads in a given block. Shared memory can&#39;t be accessed by threads not in the specified block. This is illustrated in the diagram below.</p><p><img src="../../images/gpu_memory_layout.png" alt="image"/></p><p>In the code we wrote for vector addition, we did not use shared memory. Instead we used global memory. Global memory can be accessed from all threads, regardless of what block they live in, but has the disadvantage of taking a lot longer to read from compared with shared memory. There are two main reasons we might use shared memory in a program:</p><ol><li>It can be useful to have threads which can &#39;communicate&#39; with each other via shared memory.</li><li>If we have a kernel that frequently has to read from memory, it might be quicker to have it read from shared rather than global memory (but this very much depends on your particular algorithm).</li></ol><p>Of course, there is an obvious potential disadvantage to using shared memory. Giving multiple threads the capability to read and write from the same memory is potentially powerful. However it is also potentially dangerous. Now it is possible for threads to try to write to the same location in memory simultaneously. If we want there to be a dependency between threads, where thread A reads the results written by thread B, there is no automatic guarantee that thread A will not try to read the results before thread B has written them. We need a method to synchronise threads so this type of situation can be avoided. Fortunately, such a method exists as part of CUDAnative.</p><h1><a class="nav-anchor" id="Vector-Dot-Product-1" href="#Vector-Dot-Product-1">Vector Dot Product</a></h1><p>We will use a vector dot product to explore some of the ideas introduced above. A vector dot product is when each of the elements of a vector is multiplied by the corresponding element in a second vector. Then, all of the multiplied elements are added together to give a single number as a result.</p><p>As before, we begin our script by loading the Julia packages we need to write GPU compatible code.</p><pre><code class="language-none">using CuArrays, CUDAnative, CUDAdrv</code></pre><p>Next, we need to write the kernel. It is a lot to take in, but don&#39;t worry, we will go through it step by step.</p><pre><code class="language-none">function dot(a,b,c, N, threadsPerBlock, blocksPerGrid)

    # Set up shared memory cache for this current block.
    cache = @cuDynamicSharedMem(Int64, threadsPerBlock)

    # Initialise some variables.
    tid = (threadIdx().x - 1) + (blockIdx().x - 1) * blockDim().x
    cacheIndex = threadIdx().x - 1
    temp::Int64 = 0

    # Iterate over vector to do dot product in parallel way
    while tid &lt; N
        temp += a[tid + 1] * b[tid + 1]
        tid += blockDim().x * gridDim().x
    end

    # set cache values
    cache[cacheIndex + 1] = temp

    # synchronise threads
    sync_threads()

    # In the step below, we add up all of the values stored in the cache
    i::Int = blockDim().x/2
    while i!=0
        if cacheIndex &lt; i
            cache[cacheIndex + 1] += cache[cacheIndex + i + 1]
        end
        sync_threads()
        i/=2
    end

    # cache[1] now contains the sum of vector dot product calculations done in
    # this block, so we write it to c
    if cacheIndex == 0
        c[blockIdx().x] = cache[1]
    end

    return nothing
end</code></pre><p>This is more complicated than the vector addition kernel, so let&#39;s work through it bit by bit. Let&#39;s start by focusing on the lines below:</p><pre><code class="language-none">function dot(a,b,c, N, threadsPerBlock, blocksPerGrid)

    # Set up shared memory cache for this current block.
    cache = @cuDynamicSharedMem(Int64, threadsPerBlock)</code></pre><p>Here, we are setting a variable called <code>cache</code> to the output of a function call to <code>@cuDynamicSharedMem</code>. As the comment suggests, this is required to create a cache of shared memory that can be accessed by all the threads in the current block. <code>@cuDynamicSharedMem</code> is a function from CUDAnative which allocates an array in dynamic shared memory on the GPU. The first argument specifies the type of elements in the array and the second argument specifies the dimensions of the array. So</p><pre><code class="language-none">cache = @cuDynamicSharedMem(Int64, threadsPerBlock)</code></pre><p>allocates an array in shared memory with the dimensions <code>threadsPerBlock</code>, where each element in the array is of type <code>Int64</code>.</p><p>So now we have an array of size <code>threadsPerBlock</code> in shared memory which we can fill with <code>Int64</code>s. Next we set the value of the thread index (<code>tid</code>):</p><pre><code class="language-none"># Initialise some variables.
tid = (threadIdx().x - 1) + (blockIdx().x - 1) * blockDim().x</code></pre><p>This is the first time we&#39;ve mixed up thread and block indexes in the same kernel! So what&#39;s going on?</p><p>The aim of this line of code is to generate a unique thread index for each thread. <code>threadIdx().x</code> gives the index for the current thread inside the current block. So <code>threadIdx().x</code> is not sufficient by itself because we are launching the kernel over multiple blocks. Each block has a thread with the index 1 (so <code>threadIdx().x = 1</code>), a second thread with the index 2 (<code>threadIdx().x = 2</code>) and so on, so we need a different approach to generate a unique thread index. <code>blockDim().x</code> gives number of threads in a block, which is the same for each block in a GPU. By multiplying the block index (<code>blockIdx().x</code>) and the number of threads in a block (<code>blockDim().x</code>), we count the threads in all the blocks before the one we are currently in. Then we add the thread index (<code>threadIdx().x</code>) in the current block to this total, thus generating a unique thread index for each thread across all blocks. This approach is illustrated below.</p><p>Pretty picture</p><p>A final thing to note is that we subtract one from <code>threadIdx().x</code> and <code>blockIdx().x</code>. This is because Julia is tragically a one indexed programming language. You will notice a lot of plus and minus ones in this example, they are all there for this reason and whilst you are getting your head around the core concepts you should do you best to ignore them.</p><p>Fortunately the next two lines are conceptually a lot simpler:</p><pre><code class="language-none">cacheIndex = threadIdx().x - 1
temp::Int64 = 0</code></pre><p><code>cacheIndex</code> is the index we will use to write an element to the array of shared memory we created. Remember shared memory is only accessible within the current block, so we do not need to worry about making a unique index across blocks like we did for <code>tid</code>. We set it to <code>threadIdx().x - 1</code> so that each thread is writing to a separate location in shared memory - otherwise threads could overwrite the results calculated by other threads.</p><p>Now we are ready to start calculating the dot product:</p><pre><code class="language-none"># Iterate over vector to do dot product in parallel way
while tid &lt; N
    temp += a[tid + 1] * b[tid + 1]
    tid += blockDim().x * gridDim().x
end</code></pre><p>For context, <code>N</code> is the number of elements in <code>a</code> (which is the same as the number of elements in <code>b</code>). So while <code>tid</code> less than the number of elements in <code>a</code>, we increment the value of temp by the product of <code>a[tid + 1]</code> and <code>b[tid + 1]</code> - this is the core operation in a vector dot product. Then, we increment <code>tid</code> by the number of threads in a block (<code>blockDim().x</code>) times the number of blocks in a grid (<code>gridDim().x</code>), which is the total number of threads on the GPU. This line enables us to carry out dot products for vectors which have more elements than the total number of threads on our GPU.</p><p>After exiting the while loop, we write the value calculated in temp to shared memory:</p><pre><code class="language-none"># set cache values
cache[cacheIndex + 1] = temp</code></pre><p>In the next step of the kernel, we want to sum up all the values stored in shared memory. We do this by finding the sum of all the elements in <code>cache</code>. But remember that each thread is running asynchronously - just because one thread has finished executing the line:</p><pre><code class="language-none"> cache[cacheIndex + 1] = temp
 ```

Doesn&#39;t mean that all threads have executed that line. To avoid trying to sum the elements of cache before they have all been written, we need to make the threads all pause and wait until every thread has reached the same line in the kernel. Fortunately, such a function exists as part of CUDAnative:
</code></pre><h1><a class="nav-anchor" id="synchronise-threads-1" href="#synchronise-threads-1">synchronise threads</a></h1><p>sync_threads()</p><pre><code class="language-none">
When each thread reaches this line, it pauses in its execution of the kernel until all of the threads in that block have reached the same place. Then, the threads restart again.

Now all the threads have written to shared memory, we are ready to sum the elements of cache:
</code></pre><h1><a class="nav-anchor" id="In-the-step-below,-we-add-up-all-of-the-values-stored-in-the-cache-1" href="#In-the-step-below,-we-add-up-all-of-the-values-stored-in-the-cache-1">In the step below, we add up all of the values stored in the cache</a></h1><p>i::Int = blockDim().x/2 while i!=0     if cacheIndex &lt; i         cache[cacheIndex + 1] += cache[cacheIndex + i + 1]     end     sync_threads()     i/=2 end</p><pre><code class="language-none">
Here, we initialise ```i``` as half of the total number of threads in a block. In the first iteration of the while loop, if ```cacheIndex``` is less than this number, we add the value stored at ```cache[cacheIndex + i + 1]``` to the value of ```cache[cacheIndex + 1]```. Then we synchronise the threads again, divide ```i``` by two and enter the second while loop iteration. If you work through this conceptually, you should see that provided the number of threads in a block is an even number, eventually the value at ```cache[1]``` will be equal to the sum of all the elements in ```cache```.

Now we need to write the value of ```cache[1]``` to ```c``` (remember that we can not directly return the value of ```cache[1]``` due to the requirement that the kernel must always return ```nothing```).
</code></pre><h1><a class="nav-anchor" id="cache[1]-now-contains-the-sum-of-vector-dot-product-calculations-done-in-1" href="#cache[1]-now-contains-the-sum-of-vector-dot-product-calculations-done-in-1">cache[1] now contains the sum of vector dot product calculations done in</a></h1><h1><a class="nav-anchor" id="this-block,-so-we-write-it-to-c-1" href="#this-block,-so-we-write-it-to-c-1">this block, so we write it to c</a></h1><p>if cacheIndex == 0     c[blockIdx().x] = cache[1] end</p><p>return nothing end</p><pre><code class="language-none">
And that&#39;s it! We have made it through the kernel. Now all we have to do is run the kernel on a GPU:
</code></pre><p>function main()</p><pre><code class="language-none"># Initialise variables
N::Int64 = 33 * 1024
threadsPerBlock::Int64 = 256
blocksPerGrid::Int64 = min(32, (N + threadsPerBlock - 1) / threadsPerBlock)

# Create a,b and c
a = CuArrays.CuArray(fill(0, N))
b = CuArrays.CuArray(fill(0, N))
c = CuArrays.CuArray(fill(0, blocksPerGrid))

# Fill a and b
for i in 1:N
    a[i] = i
    b[i] = 2*i
end

# Execute the kernel. Note the shmem argument - this is necessary to allocate
# space for the cache we allocate on the gpu with @cuDynamicSharedMem
@cuda blocks = blocksPerGrid threads = threadsPerBlock shmem =
(threadsPerBlock * sizeof(Int64)) dot(a,b,c, N, threadsPerBlock, blocksPerGrid)

# Copy c back from the gpu (device) to the host
c = Array(c)

local result = 0

# Sum the values in c
for i in 1:blocksPerGrid
    result += c[i]
end

# Check whether output is correct
println(&quot;Does GPU value &quot;, result, &quot; = &quot;, 2 * sum_squares(N - 1))</code></pre><p>end</p><p>main()</p><pre><code class="language-none"></code></pre><p>main()<code>starts by initialising several variables, including</code>N<code>which sets the size of</code>a<code>,</code>b<code>and</code>c<code>. We also initialise the number of threads we want the GPU to use per block and the number of blocks we want to use on the GPU. Next, we use CuArrays to create</code>a<code>,</code>b<code>and</code>c<code>and to fill</code>a<code>and</code>b<code>. Then, we use</code>@cuda``` to execute the kernel on the GPU:</p><pre><code class="language-none">@cuda blocks = blocksPerGrid threads = threadsPerBlock shmem =
(threadsPerBlock * sizeof(Int64)) dot(a,b,c, N, threadsPerBlock, blocksPerGrid)</code></pre><p>Note that in addition to setting the number of blocks and threads we want the GPU to use, we set a value for <code>shmem</code>. <code>shmem</code> describes the amount of dynamic shared memory we need to allocate for the kernel - see below for more details. Since we use <code>@cuDynamicSharedMem</code> to make an array of size <code>threadsPerBlock</code> full of <code>Int64</code>s in the kernel, we need to allocate <code>(threadsPerBlock * sizeof(Int64)</code> bytes of space in advance when we call <code>@cuda</code>.</p><p>After executing the kernel on GPU, we copy <code>c</code> back to the host (CPU). At this point, <code>c</code> is an array whose length equals the number of blocks in the grid. Each element in <code>c</code> is equal to the sum of the values calculated by the threads in a block. We need to sum the values of <code>c</code> to find the final result of the vector dot product:</p><pre><code class="language-none"># Sum the values in c
for i in 1:blocksPerGrid
    result += c[i]
end</code></pre><p>Finally, we do a sanity check to make sure the output is correct. For completeness, this is the function <code>sum_squares()</code>:</p><pre><code class="language-none">function sum_squares(x)
    return (x * (x + 1) * (2 * x + 1) / 6)
end</code></pre><p>And that is it! We now have a complete Julia script which calculates a vector dot product on a GPU, making use of shared memory and synchronisation. In the next section, we will discuss streaming.</p><h1><a class="nav-anchor" id="A-Note-on-Static-and-Dynamic-Allocation-1" href="#A-Note-on-Static-and-Dynamic-Allocation-1">A Note on Static and Dynamic Allocation</a></h1><p>In the first line of the kernel, we call <code>@cuDynamicSharedMem</code>. <code>@cuDynamicSharedMem</code> has a sister function, <code>@cuStaticSharedMem</code>. Like <code>@cuDynamicSharedMem</code>, <code>@cuStaticSharedMem</code> allocates arrays in shared memory. However unlike <code>@cuDynamicSharedMem</code>, <code>@cuStaticSharedMem</code> allocates arrays statically rather than dynamically. Memory that is statically allocated is allocated at compilation time, whereas memory that is dynamically allocated is allocated at program execution. We used <code>@cuDynamicSharedMem</code> in our example because one of the command line arguments for <code>@cuDynamicSharedMem</code> was a kernel command line argument (<code>threadsPerBlock</code>). Because the value of the kernel command line argument is not known at compilation time, dynamic rather than static memory allocation was required.</p><p>A consequence of using dynamic rather than static memory allocation was that we had to specify how much memory <code>@cuDynamicSharedMem</code> would need in our <code>@cuda</code> call. Otherwise, there is no way <code>@cuda</code> could know the correct amount of shared memory to allocate in advance, since <code>@cuDynamicSharedMem</code> does not determine how much shared memory it will need until it runs.</p><footer><hr/><a class="previous" href="../Vector_addition/"><span class="direction">Previous</span><span class="title">An Introduction to Parallelism</span></a><a class="next" href="../Streaming/"><span class="direction">Next</span><span class="title">Streaming</span></a></footer></article></body></html>
